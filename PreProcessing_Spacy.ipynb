{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOW7pYhbI5SO",
        "outputId": "49e09d8d-9be4-4dd3-cdaa-2900282d376a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ],
      "source": [
        "import spacy.cli\n",
        "spacy.cli.download(\"en_core_web_lg\")\n",
        "nlp = spacy.load(\"en_core_web_lg\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dgl\n",
        "!pip install torch_geometric\n",
        "!pip install networkx==2.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJaGVgiHI7Kc",
        "outputId": "afadfce9-9dd5-47dd-ce41-29564491c999"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: dgl in /usr/local/lib/python3.10/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.22.4)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.27.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.10.1)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.65.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from networkx>=2.1->dgl) (4.4.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2022.12.7)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.27.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.22.4)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.0.9)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.65.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.12)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: networkx==2.4 in /usr/local/lib/python3.10/dist-packages (2.4)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from networkx==2.4) (4.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:09:06.933076Z",
          "start_time": "2023-04-08T08:09:00.631841Z"
        },
        "pycharm": {
          "is_executing": true
        },
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmGSjLwiI5SP",
        "outputId": "60372d57-479b-4dc1-b342-3f31adc33b4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import networkx as nx\n",
        "import nltk\n",
        "\n",
        "import tqdm\n",
        "import en_core_web_lg\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "import spacy\n",
        "from time import time\n",
        "import dgl\n",
        "from scipy import sparse\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch_geometric.data import Data, HeteroData\n",
        "from datetime import datetime\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:21:02.166035Z",
          "start_time": "2023-04-08T08:21:01.304434Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3K_oNvvI5SQ",
        "outputId": "f3a65757-08ac-4215-ef56-333d75e88820"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-06T23:30:30.939834Z",
          "start_time": "2023-04-06T23:29:51.497747Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sq0N-fSP78lI",
        "outputId": "9d44797e-6bb0-4b41-ef63-a1e736bdd3a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUjMVDgvzVFP"
      },
      "source": [
        "# Load tweet file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:10:01.016790Z",
          "start_time": "2023-04-08T08:10:00.992629Z"
        },
        "id": "aD03dJLS7zCx"
      },
      "outputs": [],
      "source": [
        "p_part1 = '68841_tweets_multiclasses_filtered_0722_part1.npy'\n",
        "p_part2 = '68841_tweets_multiclasses_filtered_0722_part2.npy'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FR_x-TxtI5SS"
      },
      "source": [
        "# Concatenating The Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:10:03.091904Z",
          "start_time": "2023-04-08T08:10:01.643093Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "kpm2PHOx8wp2",
        "outputId": "35875a97-3405-45d5-e505-3fdba98f77fe"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e6a214c47e24>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_np_part1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_part1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_np_part2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_part2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_np_part1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_np_part2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] Data Loaded...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '68841_tweets_multiclasses_filtered_0722_part1.npy'"
          ]
        }
      ],
      "source": [
        "df_np_part1 = np.load(p_part1, allow_pickle=True)\n",
        "df_np_part2 = np.load(p_part2, allow_pickle=True)\n",
        "df_np = np.concatenate((df_np_part1, df_np_part2), axis = 0)\n",
        "print(\"[INFO] Data Loaded...\")\n",
        "\n",
        "\n",
        "df = pd.DataFrame(data=df_np, columns=[\"event_id\", \"tweet_id\", \"text\", \"user_id\", \"created_at\", \"user_loc\",\\\n",
        "    \"place_type\", \"place_full_name\", \"place_country_code\", \"hashtags\", \"user_mentions\", \"image_urls\", \"entities\", \n",
        "    \"words\", \"filtered_words\", \"sampled_words\"])\n",
        "\n",
        "print(\"[INFO] Data converted to dataframe...\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:10:03.680942Z",
          "start_time": "2023-04-08T08:10:03.668483Z"
        },
        "id": "8U5eFwSj80mm"
      },
      "outputs": [],
      "source": [
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:10:04.379271Z",
          "start_time": "2023-04-08T08:10:04.322181Z"
        },
        "id": "mKj41gFTI5ST"
      },
      "outputs": [],
      "source": [
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:10:05.168335Z",
          "start_time": "2023-04-08T08:10:05.147057Z"
        },
        "id": "Xgah06TXKnLs"
      },
      "outputs": [],
      "source": [
        "df.text = df.text.str.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:10:06.057055Z",
          "start_time": "2023-04-08T08:10:06.045979Z"
        },
        "id": "orYIk4hFLf4n"
      },
      "outputs": [],
      "source": [
        "df[\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:10:06.718544Z",
          "start_time": "2023-04-08T08:10:06.709391Z"
        },
        "id": "iS-YKLhHJiZ8"
      },
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:10:07.512813Z",
          "start_time": "2023-04-08T08:10:07.359207Z"
        },
        "id": "UuFCgSd-KLKp"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjMVLF7PI5SV"
      },
      "source": [
        "# Defining preprocessing functions for TEXT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:10:08.431957Z",
          "start_time": "2023-04-08T08:10:08.423395Z"
        },
        "id": "9Epbg4vUrBSz"
      },
      "outputs": [],
      "source": [
        "\n",
        "def clean_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = str(text).lower()\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Remove digits\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    return text\n",
        "\n",
        "def tokenize_text(text):\n",
        "    # Tokenize text using TweetTokenizer\n",
        "    tokenizer = TweetTokenizer()\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "def lemmatize_tokens(tokens):\n",
        "    # Lemmatize tokens using WordNetLemmatizer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return lemmas\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    # Remove stop words using NLTK's English stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "    return filtered_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:10:27.447426Z",
          "start_time": "2023-04-08T08:10:09.164817Z"
        },
        "id": "FxfonNxgrKeH"
      },
      "outputs": [],
      "source": [
        "# Apply preprocessing to the text column\n",
        "df['text'] = df['text'].apply(clean_text)\n",
        "df['text'] = df['text'].apply(tokenize_text)\n",
        "df['text'] = df['text'].apply(lemmatize_tokens)\n",
        "df['text'] = df['text'].apply(remove_stopwords)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:10:27.511181Z",
          "start_time": "2023-04-08T08:10:27.465605Z"
        },
        "id": "fFCoz3OrI5SV"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAj25j7-v-hv"
      },
      "source": [
        "# Convert Date and Time in Numeric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:10:27.511526Z",
          "start_time": "2023-04-08T08:10:27.510931Z"
        },
        "id": "MKABo8N_I5SW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:10:27.740803Z",
          "start_time": "2023-04-08T08:10:27.511333Z"
        },
        "id": "xcsRX4AHI5SW"
      },
      "outputs": [],
      "source": [
        "# sort data in DATAFRAME 1 by time\n",
        "df = df.sort_values(by='created_at').reset_index()\n",
        "# append date\n",
        "df['date'] = [d.date() for d in df['created_at']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:10:27.835277Z",
          "start_time": "2023-04-08T08:10:27.742380Z"
        },
        "id": "RPVYEFFLI5SW"
      },
      "outputs": [],
      "source": [
        "# df['created_at'] = pd.to_datetime(df['created_at'])\n",
        "#\n",
        "# # convert datetime column to numeric format\n",
        "# df['created_at'] = pd.to_numeric(df['created_at'].dt.strftime('%Y%m%d%H%M%S'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:10:27.835876Z",
          "start_time": "2023-04-08T08:10:27.782733Z"
        },
        "id": "MY3AoCZUI5SW"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJZpJ8VxzLGL"
      },
      "source": [
        "# Load message file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:10:27.841325Z",
          "start_time": "2023-04-08T08:10:27.783178Z"
        },
        "id": "o8Zif8Ln74cI"
      },
      "outputs": [],
      "source": [
        "load_path = 'all_df_words_ents_mids.npy'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:10:27.924594Z",
          "start_time": "2023-04-08T08:10:27.807632Z"
        },
        "id": "lTIJ7qDi9DU2"
      },
      "outputs": [],
      "source": [
        "df_np = np.load(load_path, allow_pickle=True)\n",
        "\n",
        "print(\"[INFO] Data Loaded...\")\n",
        "\n",
        "df1 = pd.DataFrame(data=df_np, \\\n",
        "    columns=['document_ids', 'sentence_ids', 'sentences', 'event_type_ids', 'words', 'unique_words', 'entities', 'message_ids'])\n",
        "\n",
        "print(\"[INFO] Data converted to dataframe...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:10:27.948532Z",
          "start_time": "2023-04-08T08:10:27.870701Z"
        },
        "id": "Ta5pRp209LMf"
      },
      "outputs": [],
      "source": [
        "print(df1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:10:27.948815Z",
          "start_time": "2023-04-08T08:10:27.914972Z"
        },
        "id": "kDMgT7-FI5SX"
      },
      "outputs": [],
      "source": [
        "df1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS4Ib9deG8XP"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:10:28.058937Z",
          "start_time": "2023-04-08T08:10:27.915417Z"
        },
        "id": "82OGPu7e9MjS"
      },
      "outputs": [],
      "source": [
        "df1.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:10:28.081889Z",
          "start_time": "2023-04-08T08:10:27.958860Z"
        },
        "id": "NQ5KN-82IMnb"
      },
      "outputs": [],
      "source": [
        "df1.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2uENap_za5X"
      },
      "source": [
        "# Convert ids into Numeric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:10:28.082123Z",
          "start_time": "2023-04-08T08:10:27.959129Z"
        },
        "id": "Ye1MgVPUxF0y"
      },
      "outputs": [],
      "source": [
        "# Create a dictionary that maps unique string values of 'document_ids' to integers\n",
        "doc_id_map = {  doc_id:i for i, doc_id in enumerate(df1['document_ids'].unique())}\n",
        "\n",
        "# Map the 'document_ids' column to integer values\n",
        "df1['document_ids'] = df1['document_ids'].map(doc_id_map)\n",
        "\n",
        "# Convert the 'document_ids' column to integer type\n",
        "df1['document_ids'] = df1['document_ids'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:10:28.086568Z",
          "start_time": "2023-04-08T08:10:27.966296Z"
        },
        "id": "JZE5KPz0IOdj"
      },
      "outputs": [],
      "source": [
        "# Convert the 'sentence_ids' column to integer type\n",
        "df1['sentence_ids'] = df1['sentence_ids'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:10:28.086850Z",
          "start_time": "2023-04-08T08:10:28.010776Z"
        },
        "id": "2YnqRLWcwsVx"
      },
      "outputs": [],
      "source": [
        "# Create a dictionary that maps unique string values of 'sentence_ids' to integers\n",
        "msg_id_map = {msg_id:i for i, msg_id in enumerate(df1['message_ids'].unique())}\n",
        "\n",
        "# Map the 'sentence_ids' column to integer values\n",
        "df1['message_ids'] = df1['message_ids'].map(msg_id_map)\n",
        "\n",
        "# Convert the 'sentence_ids' column to integer type\n",
        "df1['message_ids'] = df1['message_ids'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:10:30.886789Z",
          "start_time": "2023-04-08T08:10:28.054506Z"
        },
        "id": "DhY5NS55xtol"
      },
      "outputs": [],
      "source": [
        "# Apply preprocessing to the text column\n",
        "df1['sentences'] = df1['sentences'].apply(clean_text)\n",
        "df1['sentences'] = df1['sentences'].apply(tokenize_text)\n",
        "df1['sentences'] = df1['sentences'].apply(lemmatize_tokens)\n",
        "df1['sentences'] = df1['sentences'].apply(remove_stopwords)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:10:30.887327Z",
          "start_time": "2023-04-08T08:10:30.886710Z"
        },
        "id": "1v4CHUU5x3iw"
      },
      "outputs": [],
      "source": [
        "df1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwlQC4ycI5SZ"
      },
      "source": [
        "# FILTERING TWEETS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2qAv5buI5SZ"
      },
      "outputs": [],
      "source": [
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "U_pps0tuOBkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4XImnZQI5SZ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
        "\n",
        "# Check if a GPU is available and use it\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the pre-trained sentiment analysis model and tokenizer\n",
        "model_name = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
        "\n",
        "# Create the sentiment analysis pipeline\n",
        "sentiment_analyzer = TextClassificationPipeline(model=model, tokenizer=tokenizer, task='sentiment-analysis')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKyg8bmMI5SZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyYlRNeRI5SZ"
      },
      "outputs": [],
      "source": [
        "sentiment_analyzer(df.text[68839])[0]['score']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOS2gG3FI5Sa"
      },
      "outputs": [],
      "source": [
        "messages =  df.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgoD9QYdI5Sa"
      },
      "outputs": [],
      "source": [
        "# Compute the sentiment scores for each message\n",
        "batch_size = 64\n",
        "sentiment_scores = []\n",
        "for i in tqdm.tqdm(range(0, len(messages), batch_size)):\n",
        "    batch = messages[i:i + batch_size].apply(str).tolist()\n",
        "    inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probabilities = torch.softmax(logits, dim=-1).cpu().numpy()\n",
        "\n",
        "    for probability in probabilities:\n",
        "        sentiment_scores.append(probability[1])  # Get the score for the positive sentiment class\n",
        "\n",
        "\n",
        "# Add sentiment scores to the DataFrame\n",
        "df['sentiment_scores'] = sentiment_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0j_63lPI5Sa"
      },
      "outputs": [],
      "source": [
        "# df.to_csv(\"processed_tweets_data.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lze0cMSI5Sa"
      },
      "outputs": [],
      "source": [
        "new_DF = pd.read_csv(\"processed_tweets_data.csv\")\n",
        "new_DF['sentiment_scores'].tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osrL4s80I5Sa"
      },
      "outputs": [],
      "source": [
        "threshold = 0.5\n",
        "\n",
        "filtered_df = new_DF[new_DF['sentiment_scores'] >= threshold]\n",
        "filtered_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFmgPbRAI5Sa"
      },
      "outputs": [],
      "source": [
        "print(\"TOTAL MESSAGES: \", len(new_DF))\n",
        "print(\"FILTERED MESSAGES:\", len(filtered_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZNsB_NZI5Sb"
      },
      "outputs": [],
      "source": [
        "# # Set batch size and preallocate list for sentiment scores\n",
        "# batch_size = 256\n",
        "# sentiment_labels = []\n",
        "\n",
        "# # Process messages in batches\n",
        "# for i in tqdm.tqdm(range(0, len(messages), batch_size)):\n",
        "#     batch = messages[i:i + batch_size].apply(str).tolist()\n",
        "#     results = sentiment_analyzer(batch)\n",
        "#     for result in results:\n",
        "#         sentiment_labels.append(result['label'])\n",
        "\n",
        "# # Add sentiment scores to the DataFrame\n",
        "# df['sentiment_labels'] = sentiment_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kZdR1MiI5Sb"
      },
      "outputs": [],
      "source": [
        "filtered_df.drop(['sentiment_scores'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snhzZ_mOI5Sb"
      },
      "source": [
        "# Generating the initial message features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:10:32.055627Z",
          "start_time": "2023-04-08T08:10:32.006984Z"
        },
        "id": "B6ctXNaMI5Sb"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "The embedding of each document is the average of the pre-trained embeddings of all the words in it.\n",
        "'''\n",
        "\n",
        "def get_document_embeddings(df):\n",
        "    # Load the pre-trained English language model from Spacy library\n",
        "    nlp = en_core_web_lg.load()\n",
        "    # Apply the language model on each document and calculate its vector representation\n",
        "    embeddings = df.filtered_words.apply(lambda x: nlp(' '.join(x)).vector).values\n",
        "    # Stack all the embeddings into a numpy array and return it\n",
        "    return np.stack(embeddings, axis=0)\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "This function encodes a timestamp string in the format '2012-10-11 07:19:34' into a list of two time features.\n",
        "'''\n",
        "\n",
        "def extract_time_features(timestamp_str):\n",
        "    # Convert the timestamp string to a datetime object\n",
        "    timestamp = datetime.fromisoformat(str(timestamp_str))\n",
        "    # Define a datetime object to represent the zero time for OLE time\n",
        "    OLE_TIME_ZERO = datetime(1899, 12, 30)\n",
        "    # Calculate the time difference between the timestamp and the OLE time zero\n",
        "    delta = timestamp - OLE_TIME_ZERO\n",
        "    # Calculate the time features by normalizing the time difference into fractions of a day\n",
        "    time_features = [(float(delta.days) / 100000.), (float(delta.seconds) / 86400)] # 86,400 seconds in a day\n",
        "    # Return the time features as a list\n",
        "    return time_features\n",
        "\n",
        "'''\n",
        "This function encodes the timestamps of all the messages in the dataframe into a numpy array of time features.\n",
        "'''\n",
        "def get_time_features(df):\n",
        "    # Apply the extract_time_features function on each timestamp string in the dataframe\n",
        "    time_features = np.asarray([extract_time_features(timestamp_str) for timestamp_str in df['created_at']])\n",
        "    # Return the time features as a numpy array\n",
        "    return time_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:10:33.202670Z",
          "start_time": "2023-04-08T08:10:33.193955Z"
        },
        "id": "GDW6cxuOI5Sb"
      },
      "outputs": [],
      "source": [
        "filtered_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:15:28.351870Z",
          "start_time": "2023-04-08T08:10:33.925589Z"
        },
        "id": "9GIHq6DMI5Sc"
      },
      "outputs": [],
      "source": [
        "document_features = get_document_embeddings(df)\n",
        "print(\"Document features generated.\")\n",
        "\n",
        "document_features[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:16:39.968070Z",
          "start_time": "2023-04-08T08:16:39.922385Z"
        },
        "id": "5hdJpH_NI5Sc"
      },
      "outputs": [],
      "source": [
        "document_features[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:16:40.656010Z",
          "start_time": "2023-04-08T08:16:40.608152Z"
        },
        "id": "ytdGFSWRI5Sc"
      },
      "outputs": [],
      "source": [
        "document_features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:16:41.623234Z",
          "start_time": "2023-04-08T08:16:41.326539Z"
        },
        "id": "78g56_vYI5Sc"
      },
      "outputs": [],
      "source": [
        "time_features = get_time_features(df)\n",
        "\n",
        "print(\"Time features generated.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:16:42.223393Z",
          "start_time": "2023-04-08T08:16:42.179070Z"
        },
        "id": "S5N3zs1II5Sc"
      },
      "outputs": [],
      "source": [
        "time_features[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:16:43.007278Z",
          "start_time": "2023-04-08T08:16:42.965110Z"
        },
        "id": "71WKRI9JI5Sc"
      },
      "outputs": [],
      "source": [
        "time_features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:16:43.943371Z",
          "start_time": "2023-04-08T08:16:43.938588Z"
        },
        "id": "W6uYq_tDI5Sd"
      },
      "outputs": [],
      "source": [
        "# Concatenate the document features and time features into a single numpy array\n",
        "combined_features = np.concatenate((document_features, time_features), axis=1)\n",
        "print(\"Concatenated document features and time features.\")\n",
        "combined_features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:16:45.511230Z",
          "start_time": "2023-04-08T08:16:45.393086Z"
        },
        "id": "SVoRfPkCI5Sd"
      },
      "outputs": [],
      "source": [
        "# Save the concatenated features as a numpy array file\n",
        "save_file_path =  'Spacy_Full_features_69612_0709_spacy_lg_zero_multiclasses_filtered.npy'\n",
        "np.save(save_file_path, combined_features)\n",
        "print(\"Initial features saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:16:48.224796Z",
          "start_time": "2023-04-08T08:16:48.170732Z"
        },
        "id": "8tDMmxZiI5Sd"
      },
      "outputs": [],
      "source": [
        "# Load the concatenated features from the saved numpy array file\n",
        "loaded_features = np.load(save_file_path)\n",
        "print(\"Initial features loaded.\")\n",
        "print(loaded_features.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JffEhVMlI5Sd"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOQll9nfI5Sd"
      },
      "source": [
        "# CONSTRUCTING GRAPHS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:16:50.087936Z",
          "start_time": "2023-04-08T08:16:50.082091Z"
        },
        "id": "Rbu7N7pcI5Sd"
      },
      "outputs": [],
      "source": [
        "def construct_graph_from_df(df, G=None):\n",
        "    if G is None:\n",
        "        G = nx.Graph()\n",
        "    for _, row in df.iterrows():\n",
        "        tid = 't_' + str(row['tweet_id'])\n",
        "        G.add_node(tid)\n",
        "        G.nodes[tid]['tweet_id'] = True  # right-hand side value is irrelevant for the lookup\n",
        "\n",
        "        user_ids = row['user_mentions']\n",
        "        user_ids  = list(user_ids)\n",
        "        user_ids.append(row['user_id'])\n",
        "        user_ids = ['u_' + str(each) for each in user_ids]\n",
        "        # print(user_ids)\n",
        "        G.add_nodes_from(user_ids)\n",
        "        for each in user_ids:\n",
        "            G._node[each]['user_id'] = True\n",
        "\n",
        "        entities = row['entities']\n",
        "        # entities = ['e_' + each for each in entities]\n",
        "        # print(entities)\n",
        "        G.add_nodes_from(entities)\n",
        "        for each in entities:\n",
        "            G._node[each]['entity'] = True\n",
        "\n",
        "        words = row['sampled_words']\n",
        "        words = ['w_' + each for each in words]\n",
        "        # print(words)\n",
        "        G.add_nodes_from(words)\n",
        "        for each in words:\n",
        "            G._node[each]['word'] = True\n",
        "\n",
        "        edges = []\n",
        "        edges += [(tid, each) for each in user_ids]\n",
        "        edges += [(tid, each) for each in entities]\n",
        "        edges += [(tid, each) for each in words]\n",
        "        G.add_edges_from(edges)\n",
        "\n",
        "    return G\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:16:51.611087Z",
          "start_time": "2023-04-08T08:16:51.597943Z"
        },
        "id": "2dB1pGogI5Se"
      },
      "outputs": [],
      "source": [
        "# convert a heterogeneous social graph G to a homogeneous message graph following eq. 1 of the paper,\n",
        "# and store the sparse binary adjacency matrix of the homogeneous message graph.\n",
        "\n",
        "\n",
        "def to_dgl_graph_v3(G, save_path=None):\n",
        "    message = ''\n",
        "    print('Start converting heterogeneous networkx graph to homogeneous dgl graph.')\n",
        "    message += 'Start converting heterogeneous networkx graph to homogeneous dgl graph.\\n'\n",
        "    all_start = time()\n",
        "\n",
        "    print('\\tGetting a list of all nodes ...')\n",
        "    message += '\\tGetting a list of all nodes ...\\n'\n",
        "    start = time()\n",
        "    all_nodes = list(G.nodes)\n",
        "    mins = (time() - start) / 60\n",
        "    print('\\tDone. Time elapsed: ', mins, ' mins\\n')\n",
        "    message += '\\tDone. Time elapsed: '\n",
        "    message += str(mins)\n",
        "    message += ' mins\\n'\n",
        "\n",
        "    # print('All nodes: ', all_nodes)\n",
        "    # print('Total number of nodes: ', len(all_nodes))\n",
        "\n",
        "    print('\\tGetting adjacency matrix ...')\n",
        "    message += '\\tGetting adjacency matrix ...\\n'\n",
        "    start = time()\n",
        "    A = nx.to_numpy_matrix(G)  # Returns the graph adjacency matrix as a NumPy matrix.\n",
        "    mins = (time() - start) / 60\n",
        "    print('\\tDone. Time elapsed: ', mins, ' mins\\n')\n",
        "    message += '\\tDone. Time elapsed: '\n",
        "    message += str(mins)\n",
        "    message += ' mins\\n'\n",
        "\n",
        "    # compute commuting matrices\n",
        "    print('\\tGetting lists of nodes of various types ...')\n",
        "    message += '\\tGetting lists of nodes of various types ...\\n'\n",
        "    start = time()\n",
        "    tid_nodes = list(nx.get_node_attributes(G, 'tweet_id').keys())\n",
        "    userid_nodes = list(nx.get_node_attributes(G, 'user_id').keys())\n",
        "    word_nodes = list(nx.get_node_attributes(G, 'word').keys())\n",
        "    entity_nodes = list(nx.get_node_attributes(G, 'entity').keys())\n",
        "    del G\n",
        "    mins = (time() - start) / 60\n",
        "    print('\\tDone. Time elapsed: ', mins, ' mins\\n')\n",
        "    message += '\\tDone. Time elapsed: '\n",
        "    message += str(mins)\n",
        "    message += ' mins\\n'\n",
        "\n",
        "    print('\\tConverting node lists to index lists ...')\n",
        "    message += '\\tConverting node lists to index lists ...\\n'\n",
        "    start = time()\n",
        "    #  find the index of target nodes in the list of all_nodes\n",
        "    indices_tid = [all_nodes.index(x) for x in tid_nodes]\n",
        "    indices_userid = [all_nodes.index(x) for x in userid_nodes]\n",
        "    indices_word = [all_nodes.index(x) for x in word_nodes]\n",
        "    indices_entity = [all_nodes.index(x) for x in entity_nodes]\n",
        "    del tid_nodes\n",
        "    del userid_nodes\n",
        "    del word_nodes\n",
        "    del entity_nodes\n",
        "    mins = (time() - start) / 60\n",
        "    print('\\tDone. Time elapsed: ', mins, ' mins\\n')\n",
        "    message += '\\tDone. Time elapsed: '\n",
        "    message += str(mins)\n",
        "    message += ' mins\\n'\n",
        "\n",
        "    # ----------------------tweet-user-tweet----------------------\n",
        "    print('\\tStart constructing tweet-user-tweet commuting matrix ...')\n",
        "    print('\\t\\t\\tStart constructing tweet-user matrix ...')\n",
        "    message += '\\tStart constructing tweet-user-tweet commuting matrix ...\\n\\t\\t\\tStart constructing tweet-user ' \\\n",
        "               'matrix ...\\n '\n",
        "    start = time()\n",
        "    w_tid_userid = A[np.ix_(indices_tid, indices_userid)]\n",
        "    #  return a N(indices_tid)*N(indices_userid) matrix, representing the weight of edges between tid and userid\n",
        "    mins = (time() - start) / 60\n",
        "    print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n",
        "    message += '\\t\\t\\tDone. Time elapsed: '\n",
        "    message += str(mins)\n",
        "    message += ' mins\\n'\n",
        "\n",
        "    # convert to scipy sparse matrix\n",
        "    print('\\t\\t\\tConverting to sparse matrix ...')\n",
        "    message += '\\t\\t\\tConverting to sparse matrix ...\\n'\n",
        "    start = time()\n",
        "    s_w_tid_userid = sparse.csr_matrix(w_tid_userid)  # matrix compression\n",
        "    del w_tid_userid\n",
        "    mins = (time() - start) / 60\n",
        "    print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n",
        "    message += '\\t\\t\\tDone. Time elapsed: '\n",
        "    message += str(mins)\n",
        "    message += ' mins\\n'\n",
        "\n",
        "    print('\\t\\t\\tTransposing ...')\n",
        "    message += '\\t\\t\\tTransposing ...\\n'\n",
        "    start = time()\n",
        "    s_w_userid_tid = s_w_tid_userid.transpose()\n",
        "    mins = (time() - start) / 60\n",
        "    print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n",
        "    message += '\\t\\t\\tDone. Time elapsed: '\n",
        "    message += str(mins)\n",
        "    message += ' mins\\n'\n",
        "\n",
        "    print('\\t\\t\\tCalculating tweet-user * user-tweet ...')\n",
        "    message += '\\t\\t\\tCalculating tweet-user * user-tweet ...\\n'\n",
        "    start = time()\n",
        "    s_m_tid_userid_tid = s_w_tid_userid * s_w_userid_tid  # homogeneous message graph\n",
        "    mins = (time() - start) / 60\n",
        "    print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n",
        "    message += '\\t\\t\\tDone. Time elapsed: '\n",
        "    message += str(mins)\n",
        "    message += ' mins\\n'\n",
        "\n",
        "    print('\\t\\t\\tSaving ...')\n",
        "    message += '\\t\\t\\tSaving ...\\n'\n",
        "    start = time()\n",
        "    if save_path is not None:\n",
        "        sparse.save_npz(save_path + \"s_m_tid_userid_tid.npz\", s_m_tid_userid_tid)\n",
        "        print(\"Sparse binary userid commuting matrix saved.\")\n",
        "        del s_m_tid_userid_tid\n",
        "    del s_w_tid_userid\n",
        "    del s_w_userid_tid\n",
        "    mins = (time() - start) / 60\n",
        "    print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n",
        "    message += '\\t\\t\\tDone. Time elapsed: '\n",
        "    message += str(mins)\n",
        "    message += ' mins\\n'\n",
        "\n",
        "    # ----------------------tweet-ent-tweet------------------------\n",
        "    print('\\tStart constructing tweet-ent-tweet commuting matrix ...')\n",
        "    print('\\t\\t\\tStart constructing tweet-ent matrix ...')\n",
        "    message += '\\tStart constructing tweet-ent-tweet commuting matrix ...\\n\\t\\t\\tStart constructing tweet-ent matrix ' \\\n",
        "               '...\\n '\n",
        "    start = time()\n",
        "    w_tid_entity = A[np.ix_(indices_tid, indices_entity)]\n",
        "    mins = (time() - start) / 60\n",
        "    print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n",
        "    message += '\\t\\t\\tDone. Time elapsed: '\n",
        "    message += str(mins)\n",
        "    message += ' mins\\n'\n",
        "\n",
        "    # convert to scipy sparse matrix\n",
        "    print('\\t\\t\\tConverting to sparse matrix ...')\n",
        "    message += '\\t\\t\\tConverting to sparse matrix ...\\n'\n",
        "    start = time()\n",
        "    s_w_tid_entity = sparse.csr_matrix(w_tid_entity)\n",
        "    del w_tid_entity\n",
        "    mins = (time() - start) / 60\n",
        "    print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n",
        "    message += '\\t\\t\\tDone. Time elapsed: '\n",
        "    message += str(mins)\n",
        "    message += ' mins\\n'\n",
        "\n",
        "    print('\\t\\t\\tTransposing ...')\n",
        "    message += '\\t\\t\\tTransposing ...\\n'\n",
        "    start = time()\n",
        "    s_w_entity_tid = s_w_tid_entity.transpose()\n",
        "    mins = (time() - start) / 60\n",
        "    print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n",
        "    message += '\\t\\t\\tDone. Time elapsed: '\n",
        "    message += str(mins)\n",
        "    message += ' mins\\n'\n",
        "\n",
        "    print('\\t\\t\\tCalculating tweet-ent * ent-tweet ...')\n",
        "    message += '\\t\\t\\tCalculating tweet-ent * ent-tweet ...\\n'\n",
        "    start = time()\n",
        "    s_m_tid_entity_tid = s_w_tid_entity * s_w_entity_tid\n",
        "    mins = (time() - start) / 60\n",
        "    print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n",
        "    message += '\\t\\t\\tDone. Time elapsed: '\n",
        "    message += str(mins)\n",
        "    message += ' mins\\n'\n",
        "\n",
        "    print('\\t\\t\\tSaving ...')\n",
        "    message += '\\t\\t\\tSaving ...\\n'\n",
        "    start = time()\n",
        "    if save_path is not None:\n",
        "        sparse.save_npz(save_path + \"s_m_tid_entity_tid.npz\", s_m_tid_entity_tid)\n",
        "        print(\"Sparse binary entity commuting matrix saved.\")\n",
        "        del s_m_tid_entity_tid\n",
        "    del s_w_tid_entity\n",
        "    del s_w_entity_tid\n",
        "    mins = (time() - start) / 60\n",
        "    print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n",
        "    message += '\\t\\t\\tDone. Time elapsed: '\n",
        "    message += str(mins)\n",
        "    message += ' mins\\n'\n",
        "\n",
        "    # ----------------------tweet-word-tweet----------------------\n",
        "    print('\\tStart constructing tweet-word-tweet commuting matrix ...')\n",
        "    print('\\t\\t\\tStart constructing tweet-word matrix ...')\n",
        "    message += '\\tStart constructing tweet-word-tweet commuting matrix ...\\n\\t\\t\\tStart constructing tweet-word ' \\\n",
        "               'matrix ...\\n '\n",
        "    start = time()\n",
        "    w_tid_word = A[np.ix_(indices_tid, indices_word)]\n",
        "    del A\n",
        "    mins = (time() - start) / 60\n",
        "    print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n",
        "    message += '\\t\\t\\tDone. Time elapsed: '\n",
        "    message += str(mins)\n",
        "    message += ' mins\\n'\n",
        "\n",
        "    # convert to scipy sparse matrix\n",
        "    print('\\t\\t\\tConverting to sparse matrix ...')\n",
        "    message += '\\t\\t\\tConverting to sparse matrix ...\\n'\n",
        "    start = time()\n",
        "    s_w_tid_word = sparse.csr_matrix(w_tid_word)\n",
        "    del w_tid_word\n",
        "    mins = (time() - start) / 60\n",
        "    print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n",
        "    message += '\\t\\t\\tDone. Time elapsed: '\n",
        "    message += str(mins)\n",
        "    message += ' mins\\n'\n",
        "\n",
        "    print('\\t\\t\\tTransposing ...')\n",
        "    message += '\\t\\t\\tTransposing ...\\n'\n",
        "    start = time()\n",
        "    s_w_word_tid = s_w_tid_word.transpose()\n",
        "    mins = (time() - start) / 60\n",
        "    print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n",
        "    message += '\\t\\t\\tDone. Time elapsed: '\n",
        "    message += str(mins)\n",
        "    message += ' mins\\n'\n",
        "\n",
        "    print('\\t\\t\\tCalculating tweet-word * word-tweet ...')\n",
        "    message += '\\t\\t\\tCalculating tweet-word * word-tweet ...\\n'\n",
        "    start = time()\n",
        "    s_m_tid_word_tid = s_w_tid_word * s_w_word_tid\n",
        "    mins = (time() - start) / 60\n",
        "    print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n",
        "    message += '\\t\\t\\tDone. Time elapsed: '\n",
        "    message += str(mins)\n",
        "    message += ' mins\\n'\n",
        "\n",
        "    print('\\t\\t\\tSaving ...')\n",
        "    message += '\\t\\t\\tSaving ...\\n'\n",
        "    start = time()\n",
        "    if save_path is not None:\n",
        "        sparse.save_npz(save_path + \"s_m_tid_word_tid.npz\", s_m_tid_word_tid)\n",
        "        print(\"Sparse binary word commuting matrix saved.\")\n",
        "        del s_m_tid_word_tid\n",
        "    del s_w_tid_word\n",
        "    del s_w_word_tid\n",
        "    mins = (time() - start) / 60\n",
        "    print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n",
        "    message += '\\t\\t\\tDone. Time elapsed: '\n",
        "    message += str(mins)\n",
        "    message += ' mins\\n'\n",
        "\n",
        "    # ----------------------compute tweet-tweet adjacency matrix----------------------\n",
        "    print('\\tComputing tweet-tweet adjacency matrix ...')\n",
        "    message += '\\tComputing tweet-tweet adjacency matrix ...\\n'\n",
        "    start = time()\n",
        "    if save_path is not None:\n",
        "        s_m_tid_userid_tid = sparse.load_npz(save_path + \"s_m_tid_userid_tid.npz\")\n",
        "        print(\"Sparse binary userid commuting matrix loaded.\")\n",
        "        s_m_tid_entity_tid = sparse.load_npz(save_path + \"s_m_tid_entity_tid.npz\")\n",
        "        print(\"Sparse binary entity commuting matrix loaded.\")\n",
        "        s_m_tid_word_tid = sparse.load_npz(save_path + \"s_m_tid_word_tid.npz\")\n",
        "        print(\"Sparse binary word commuting matrix loaded.\")\n",
        "\n",
        "    s_A_tid_tid = s_m_tid_userid_tid + s_m_tid_entity_tid\n",
        "    del s_m_tid_userid_tid\n",
        "    del s_m_tid_entity_tid\n",
        "    s_bool_A_tid_tid = (s_A_tid_tid + s_m_tid_word_tid).astype('bool')  # confirm the connect between tweets\n",
        "    del s_m_tid_word_tid\n",
        "    del s_A_tid_tid\n",
        "    mins = (time() - start) / 60\n",
        "    print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n",
        "    message += '\\t\\t\\tDone. Time elapsed: '\n",
        "    message += str(mins)\n",
        "    message += ' mins\\n'\n",
        "    all_mins = (time() - all_start) / 60\n",
        "    print('\\tOver all time elapsed: ', all_mins, ' mins\\n')\n",
        "    message += '\\tOver all time elapsed: '\n",
        "    message += str(all_mins)\n",
        "    message += ' mins\\n'\n",
        "\n",
        "    if save_path is not None:\n",
        "        sparse.save_npz(save_path + \"s_bool_A_tid_tid.npz\", s_bool_A_tid_tid)\n",
        "        print(\"Sparse binary adjacency matrix saved.\")\n",
        "        s_bool_A_tid_tid = sparse.load_npz(save_path + \"s_bool_A_tid_tid.npz\")\n",
        "        print(\"Sparse binary adjacency matrix loaded.\")\n",
        "\n",
        "    # create corresponding dgl graph\n",
        "    G = dgl.DGLGraph(s_bool_A_tid_tid)\n",
        "    print('We have %d nodes.' % G.number_of_nodes())\n",
        "    print('We have %d edges.' % G.number_of_edges())\n",
        "    print()\n",
        "    message += 'We have '\n",
        "    message += str(G.number_of_nodes())\n",
        "    message += ' nodes.'\n",
        "    message += 'We have '\n",
        "    message += str(G.number_of_edges())\n",
        "    message += ' edges.\\n'\n",
        "\n",
        "    return all_mins, message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:16:52.369487Z",
          "start_time": "2023-04-08T08:16:52.323673Z"
        },
        "id": "Hh27CtSAI5Se"
      },
      "outputs": [],
      "source": [
        "def construct_incremental_dataset_0922(df, save_path, features, test=True):\n",
        "    # If test equals true, construct the initial graph using test_ini_size tweets\n",
        "    # and increment the graph by test_incr_size tweets each day\n",
        "    test_ini_size = 500\n",
        "    test_incr_size = 100\n",
        "\n",
        "    # save data splits for training/validate/test mask generation\n",
        "    data_split = []\n",
        "    # save time spent for the heterogeneous -> homogeneous conversion of each graph\n",
        "    all_graph_mins = []\n",
        "    message = \"\"\n",
        "    # extract distinct dates\n",
        "    distinct_dates = df.date.unique()  # 2012-11-07\n",
        "    # print(\"Distinct dates: \", distinct_dates)\n",
        "    print(\"Number of distinct dates: \", len(distinct_dates))\n",
        "    print()\n",
        "    message += \"Number of distinct dates: \"\n",
        "    message += str(len(distinct_dates))\n",
        "    message += \"\\n\"\n",
        "\n",
        "    # split data by dates and construct graphs\n",
        "    # first week -> initial graph (20254 tweets)\n",
        "    print(\"Start constructing initial graph ...\")\n",
        "    message += \"\\nStart constructing initial graph ...\\n\"\n",
        "    ini_df = df.loc[df['date'].isin(distinct_dates[:7])]  # find top 7 dates\n",
        "    if test:\n",
        "        ini_df = ini_df[:test_ini_size]  # top test_ini_size dates\n",
        "    G = construct_graph_from_df(ini_df)\n",
        "    path = save_path + '0/'\n",
        "    os.mkdir(path)\n",
        "    grap_mins, graph_message = to_dgl_graph_v3(G, save_path=path)\n",
        "    message += graph_message\n",
        "    print(\"Initial graph saved\")\n",
        "    message += \"Initial graph saved\\n\"\n",
        "    # record the total number of tweets\n",
        "    data_split.append(ini_df.shape[0])\n",
        "    # record the time spent for graph conversion\n",
        "    all_graph_mins.append(grap_mins)\n",
        "    # extract and save the labels of corresponding tweets\n",
        "    y = ini_df['event_id'].values\n",
        "    y = [int(each) for each in y]\n",
        "    np.save(path + 'labels.npy', np.asarray(y))\n",
        "    print(\"Labels saved.\")\n",
        "    message += \"Labels saved.\\n\"\n",
        "    # extract and save the features of corresponding tweets\n",
        "    indices = ini_df['index'].values.astype(np.int8).tolist()\n",
        "    x = features[indices, :]\n",
        "    np.save(path + 'features.npy', x)\n",
        "    print(\"Features saved.\")\n",
        "    message += \"Features saved.\\n\\n\"\n",
        "\n",
        "    # subsequent days -> insert tweets day by day (skip the last day because it only contains one tweet)\n",
        "    for i in range(7, len(distinct_dates) - 1):\n",
        "        print(\"Start constructing graph \", str(i - 6), \" ...\")\n",
        "        message += \"\\nStart constructing graph \"\n",
        "        message += str(i - 6)\n",
        "        message += \" ...\\n\"\n",
        "        incr_df = df.loc[df['date'] == distinct_dates[i]]\n",
        "        if test:\n",
        "            incr_df = incr_df[:test_incr_size]\n",
        "\n",
        "        # All/Relevant Message Strategy: keeping all the messages when constructing the graphs\n",
        "        # (for the Relevant Message Strategy, the unrelated messages will be removed from the graph later on).\n",
        "        # G = construct_graph_from_df(incr_df, G)\n",
        "\n",
        "        # Latest Message Strategy: construct graph using only the data of the day\n",
        "        G = construct_graph_from_df(incr_df)\n",
        "\n",
        "        path = save_path + str(i - 6) + '/'\n",
        "        os.mkdir(path)\n",
        "        grap_mins, graph_message = to_dgl_graph_v3(G, save_path=path)\n",
        "        message += graph_message\n",
        "        print(\"Graph \", str(i - 6), \" saved\")\n",
        "        message += \"Graph \"\n",
        "        message += str(i - 6)\n",
        "        message += \" saved\\n\"\n",
        "        # record the total number of tweets\n",
        "        data_split.append(incr_df.shape[0])\n",
        "        # record the time spent for graph conversion\n",
        "        all_graph_mins.append(grap_mins)\n",
        "        # extract and save the labels of corresponding tweets\n",
        "        # y = np.concatenate([y, incr_df['event_id'].values], axis = 0)\n",
        "        y = [int(each) for each in incr_df['event_id'].values]\n",
        "        np.save(path + 'labels.npy', y)\n",
        "        print(\"Labels saved.\")\n",
        "        message += \"Labels saved.\\n\"\n",
        "        # extract and save the features of corresponding tweets\n",
        "        indices = incr_df['index'].values.astype(np.int8).tolist()\n",
        "        x = features[indices, :]\n",
        "        # x = np.concatenate([x, x_incr], axis = 0)\n",
        "        np.save(path + 'features.npy', x)\n",
        "        print(\"Features saved.\")\n",
        "        message += \"Features saved.\\n\"\n",
        "\n",
        "    return message, data_split, all_graph_mins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-syuNmaI5Sf"
      },
      "outputs": [],
      "source": [
        "len(loaded_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-08T08:18:58.125644Z",
          "start_time": "2023-04-08T08:18:57.404142Z"
        },
        "id": "GnyOHf5JI5Sf"
      },
      "outputs": [],
      "source": [
        "# load features\n",
        "# the dimension of feature is 300 in this dataset\n",
        "\n",
        "# loaded_features = np.load('features_69612_0709_spacy_lg_zero_multiclasses_filtered.npy')\n",
        "save_path = 'data/'\n",
        "\n",
        "# generate test graphs, features, and labels\n",
        "message, data_split, all_graph_mins = construct_incremental_dataset_0922(df, save_path, loaded_features, True)\n",
        "with open(\"spacy_full_node_edge_statistics.txt\", \"w\") as text_file:\n",
        "    text_file.write(message)\n",
        "\n",
        "np.save('spacy_full_data_split.npy', np.asarray(data_split))\n",
        "print(\"Data split: \", data_split)\n",
        "np.save('spacy_full_all_graph_mins.npy', np.asarray(all_graph_mins))\n",
        "print(\"Time sepnt on heterogeneous -> homogeneous graph conversions: \", all_graph_mins)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0wKP0ITI5Sf"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQNSFfAXI5Sf"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEJ7m3SxI5Sh"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8Cc6VQSI5Sh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHr_RWfvI5Sh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48Rz4SdwI5Sh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPgdWXYYI5Si"
      },
      "outputs": [],
      "source": [
        "# # Load the homogenous message passing graph\n",
        "# graph = nx.read_gpickle(\"data_filtered/0/fea\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAG0G-VgI5Si"
      },
      "outputs": [],
      "source": [
        "# Define a node size function based on the node degree\n",
        "node_sizes = [d * 10 for n, d in graph.degree()]\n",
        "\n",
        "# Plot the graph with custom node color and size\n",
        "nx.draw(graph, with_labels=False, node_color='blue', node_size=node_sizes)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}